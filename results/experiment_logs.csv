timestamp,architecture,hidden_dim,num_layers,dropout,learning_rate,batch_size,n_epochs,train_size,dev_size,final_train_loss,final_dev_loss,best_dev_loss,best_epoch,optimizer,notes
2025-11-07_12-08-20,"lstm: LSTM(2, 64, batch_first=True, dropout=0.3) → fc: Sequential(  (0): Linear(in_features=64, out_features=64, bias=True)  (1): ReLU()  (2): Dropout(p=0.3, inplace=False)  (3): Linear(in_features=64, out_features=2, bias=True)) → norm: LayerNorm((64,), eps=1e-05, elementwise_affine=True)",64,1,0.3,0.001,8,50,426,92,0.702152,1.341566,0.998132,2,Adam, 
2025-11-07_14-45-47,"lstm: LSTM(2, 64, num_layers=2, batch_first=True, dropout=0.3) → fc: Sequential(  (0): Linear(in_features=64, out_features=128, bias=True)  (1): ReLU()  (2): Dropout(p=0.3, inplace=False)  (3): Linear(in_features=128, out_features=64, bias=True)  (4): ReLU()  (5): Dropout(p=0.3, inplace=False)  (6): Linear(in_features=64, out_features=2, bias=True)) → norm: LayerNorm((64,), eps=1e-05, elementwise_affine=True)",64,2,0.3,0.001,8,50,426,92,1.007214,1.024324,0.976938,3,Adam, 
